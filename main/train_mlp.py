import os 
import sys 
import random 

import torch 
from stable_baselines3.common.monitor import Monitor 
from stable_baselines3.common.vec_env import SubprocVecEnv 
from stable_baselines3.common.callbacks import CheckpointCallback 

from sb3_contrib import MaskablePPO 
from sb3_contrib.common.wrappers import ActionMasker 


from snake_game_custom_wrapper_mlp import SnakeEnv

NUM_ENV = 32 
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True) 



# linear scheduler 
def linear_schedule(initial_value, final_value=0.0): 
    
    if isinstance(initial_value, str): 
        initial_value = float(initial_value) 
        final_value = float(final_value) 
        assert (initial_value > 0.0 )

    def scheduler(progress): 
        return final_value + progress * (initial_value - final_value) 
    
    return scheduler


def make_env(seed=0):
    def _init():
        env = SnakeEnv(seed=seed)
        env = ActionMasker(env, SnakeEnv.get_action_mask)
        env = Monitor(env) 
        env.seed(seed)
        return env 
    
    return _init


def main():

    # generate a list of random seeds for each environmetn 
    seed_set = set() 
    while len(seed_set) < NUM_ENV:
        seed_set.add(random.randint(0, 1e9))

    # create the snake environment 
    env = SubprocVecEnv([make_env(seed=s) for s in seed_set])

    lr_schedule = linear_schedule(2.5e-4, 2.5e-6)
    clip_range_schedule = linear_schedule(0.15, 0.025) 

    # # instantiate a PPO agent 
    model = MaskablePPO(
        "MlpPolicy",
        env, 
        device="mps", #"cuda", 
        verbose=1, 
        n_steps=2048, 
        batch_size=512,
        n_epochs=4, 
        gamma=0.94,
        learning_rate=lr_schedule, 
        clip_range=clip_range_schedule, 
        tensorboard_log=LOG_DIR
    )

    # set the save directory 
    save_dir = "trained_models_mlp"
    os.makedirs(save_dir, exist_ok=True) 

    checkpoint_interval = 15625 # checkpoint_interval * num_envs = total_steps_per_checkpoint 
    checkpoint_callback = CheckpointCallback(save_freq=checkpoint_interval, save_path=save_dir, name_prefix="ppo_snake")

    # writing the training logs from stdout to a file 
    original_stdout = sys.stdout 
    log_file_path = os.path.join(save_dir, "training_log.txt")
    with open(log_file_path, 'w') as log_file: 
        sys.stdout = log_file 

        model.learn( 
            total_timesteps=int(10000000),
            callback=[checkpoint_callback]
        )
        env.close() 

    # restore stdout 
    sys.stdout = original_stdout 

    # svae the final model 
    model.save(os.path.join(save_dir, "ppo_snake_final.zip"))


if __name__ == "__main__": 
    main()